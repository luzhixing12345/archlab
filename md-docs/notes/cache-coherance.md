
# cache-coherance

## 多处理器体系结构

提高硬件性能最简单,最便宜的方法之一是在主板上放置多个 CPU.这可以通过让不同的 CPU 承担不同的作业(非对称多处理)或让它们全部并行运行来完成相同的作业(对称多处理,又名 SMP)来完成.有效地进行非对称多处理需要有关计算机应执行的任务的专业知识,而这在 Linux 等通用操作系统中是不可用的.另一方面,对称多处理相对容易实现.

> 相对容易但并不是真的很容易.在对称多处理环境中,CPU 共享相同的内存,因此在一个 CPU 中运行的代码可能会影响另一个 CPU 使用的内存.无法再确定在上一行中设置为某个值的变量仍然具有该值;显然,这样的编程是不可能的.

Symmetrical Multi-Processing,简称SMP,即对称多处理技术,是指将**多CPU汇集在同一总线上,各CPU间进行内存和总线共享的技术**.将同一个工作平衡地(run in parallel)分布到多个CPU上运行,该相同任务在不同CPU上共享着相同的物理内存.

与 SMP 相对应的还有一个叫做 AMP(Asymmetric Multiprocessing), 即非对称多处理器架构的概念.

- SMP的多个处理器都是同构的,使用相同架构的CPU;而AMP的多个处理器则可能是异构的.
- SMP的多个处理器共享同一内存地址空间;而AMP的每个处理器则拥有自己独立的地址空间.
- SMP的多个处理器操通常共享一个操作系统的实例;而AMP的每个处理器可以有或者没有运行操作系统, 运行操作系统的CPU也是在运行多个独立的实例.
- SMP的多处理器之间可以通过共享内存来协同通信;而AMP则需要提供一种处理器间的通信机制.

现今主流的x86多处理器服务器都是SMP架构的, 而很多嵌入式系统则是AMP架构的

在现行的SMP架构中,发展出三种模型:UMA,NUMA和COMA.下面将简单介绍 UMA 和 NUMA 两种模型, 关于 NUMA 的详细内容见 [NUMA](https://luzhixing12345.github.io/klinux/articles/mm/NUMA/)

> 下文讨论的 CPU 是指物理 CPU , 而不是多核 CPU

### UMA

Uniform Memory Access,简称UMA, 即均匀存储器存取模型.**所有处理器对所有内存有相等的访问时间**

![20240119232539](https://raw.githubusercontent.com/learner-lu/picbed/master/20240119232539.png)

既然要连接多个 CPU 和内存, 这种 UMA 的方式很明显是最简单直接的, 但问题也同样明显, **BUS 会成为性能的杀手**. **多个 CPU 需要平分总线的带宽, 这显然非常不利于计算**.

x86多处理器发展历史上,早期的多核和多处理器系统都是UMA架构的.这种架构下, 多个CPU通过同一个北桥(North Bridge)芯片与内存链接.北桥芯片里集成了内存控制器(Memory Controller),

下图是一个典型的早期 x86 UMA 系统,四路处理器通过 FSB (前端系统总线, Front Side Bus) 和主板上的内存控制器芯片 (MCH, Memory Controller Hub) 相连, CPU 通过 PCH 访问内存, DRAM 是以 UMA 方式组织的,延迟并无访问差异. 

![image](https://raw.githubusercontent.com/learner-lu/picbed/master/numa-fsb-3.png)

### NUMA

基于总线的计算机系统有一个瓶颈, 有限的带宽会导致可伸缩性问题.系统中添加的CPU越多,每个节点可用的带宽就越少.此外,添加的CPU越多,总线就越长, 延迟也就越高.

因此在另一种设计方法中, 多处理器采用物理分布式存储器, 称为**分布式共享存储器(distribute share memory, DSM)**. 为了支持更多的处理器, 存储器必须分散在处理器之间, 而不应当是集中式的; 否则, 存储器系统就无法在不大幅延长访问延迟的情况下为大量处理器提供带宽支持. 随着处理器性能的快速提高以及处理器存储器带宽需求的相应增加, 多处理器大多采用分布式存储器的方式

![20240120202032](https://raw.githubusercontent.com/learner-lu/picbed/master/20240120202032.png)

将存储器分散在节点之间, 既增加了带宽, 也缩短了到本地存储器的延迟. DSM 多处理器也被称为 NUMA(非一致存储器访问), 这是因为它的**访问时间取决于数据字在存储器的位置.** DSM 的关键缺点是处理器之间传送数据的过程变得复杂了一些, 需要在软件中多花一些力气, 以充分利用分布式存储器提升的存储器带宽.

在 SMP(对称多处理技术) 和 DSM(分布式共享存储器) 这两种体系结构中, **线程之间的通信是通过共享地址空间完成的, 存储器的地址统一编码, 任何一个拥有正确寻址权限的处理器都可以向任意存储器位置发出存储器引用**. 共享存储器的含义就是指共享地址空间.

与UMA不同的是,**在NUMA中每个处理器有属于自己的本地物理内存(local memory),对于其他CPU来说是远程物理内存(remote memory)**.一般而言,访问本地物理内存由于路径更短,其访存时间要更短.

## 缓存一致性 

人们发现使用大型的多级缓存可以充分降低处理器对于存储器带宽的需求, 但是对于存储器的访问是非对称的: 对本地存储器的访问要更快一些, 对远程存储器的访问要慢一些. 在多核心结构中, 存储器由一个芯片上的所有核心共享, 但是从一个多核心的存储器访问另一个多核心的存储器时仍然是非对称的.

采用对称共享存储器的计算机通常支持对**共享数据(share data)和专用数据(private data)**的缓存. 专用数据共单个处理器使用, 而共享数据则由多个处理器使用, 通过读写共享数据来实现处理器之间的通信.

在缓存专用数据时, 它的位置会被移动到本地缓存, 以缩短平均访问事件并降低所需要的存储器带宽, 并且由于没有其他处理器使用该数据, 程序的行为和单处理器行为相同. 在缓存共享数据时, 可能会在多个缓存中复制共享值, 这种复制过程可以减少多个处理器同时读取共享数据的争用. 但是共享数据的缓存也引发了一个新的问题: **缓存一致性**

### 缓存一致性问题

缓存策略总是伴随着数据一致性的问题,通俗的讲是**不同存储节点中同一条数据副本之间不一致的问题**.CPU Cache的存在导致多核CPU中缓存数据与内存数据之间可能存在不一致的情况.

首先思考单核CPU下,何时将缓存数据的修改同步至内存中,使得缓存与内存数据一致?

- **写直达**:CPU每次访问修改数据时,无论数据在不在缓存中,都将修改后的数据同步到内存中,缓存数据与内存数据保持强一致性,这种做法影响写操作的性能.
- **写回**:为了避免每次写操作都要进行数据同步带来的性能损失,写回策略里发生读写操作时:
  - 如果缓存行中命中了数据,写操作对缓存行中数据进行更新,并标记该缓存行为已修改.
  - 如果缓存中未命中数据,且数据所对应的缓存行中存放了其他数据:
    - **若该缓存行被标记为已修改**,读写操作都会将缓存行中现存的数据写回内存中,再将当前要获取的数据从内存读到缓存行,写操作对数据进行更新后标记该缓存行为已修改;
    - **若该缓存行未被标记为已修改**,读写操作都直接将当前要获取的数据从内存读到缓存行.写操作对数据进行更新后标记该缓存行为已修改.

假设 CPU1 和 CPU2 同时运行两个线程,都操作共同的变量 a 和 b, 为了考虑性能,使用了我们前面所说的写回策略, 把执行结果直接写入到 L1/L2 Cache 中,然后把 L1/L2 Cache 中对应的 Block 标记为脏的,这个时候**数据其实没有被同步到内存**中的,因为写回策略只有在 A 号核心中的这个 Cache Block 要被替换的时候,数据才会写入到内存里.

![20240120203920](https://raw.githubusercontent.com/learner-lu/picbed/master/20240120203920.png)

由于 CPU 1/2 的缓存策略, 导致数据在这个时候是不一致,从而可能会导致执行结果的错误.

那么,要解决这一问题,就需要一种机制,来**同步两个不同核心里面的缓存数据**.要实现的这个机制的话,要保证做到下面这 2 点:

1. 某个 CPU 核心里的 Cache 数据更新时,必须要传播到其他核心的 Cache, 称为**写传播**(Write Propagation);
2. 某个 CPU 核心里对数据的操作顺序,必须在其他核心看起来顺序是一样的, 称为**事务的串形化**(Transaction Serialization).

第一点写传播很容易就理解,当某个核心在 Cache 更新了数据,就需要同步到其他核心的 Cache 里; 第二点事务的串行化指的是不同 CPU 要看到**相同顺序的数据变化**,比如两个线程同时执行 `a=100` 和 `a=200`, 所有其他核心收到的更新变化都应该是相同的, 比如变量 a 都是先变成 100,再变成 200

![20240120210511](https://raw.githubusercontent.com/learner-lu/picbed/master/20240120210511.png)

要实现事务串形化,要做到 2 点:

- CPU 核心对于 Cache 中数据的操作,需要**同步给其他 CPU 核心**;
- 要引入「**锁**」的概念,如果两个 CPU 核心里有相同数据的 Cache,那么对于这个 Cache 数据的更新,只有拿到了「锁」,才能进行对应的数据更新.

### 总线嗅探

写传播的原则就是当某个 CPU 核心更新了 Cache 中的数据,要把该事件广播通知到其他核心.最常见实现的方式是**总线嗅探(Bus Snooping)**.

当任何一个 CPU 核心修改了 L1 Cache 中变量的值, 都会通过总线把这个事件广播通知给其他所有的核心. 每个 CPU 核心都会监听总线上的广播事件,并检查是否有相同的数据在自己的 L1 Cache 里面,如果其他 CPU 核心的 L1 Cache 中有该数据,那么也需要把该数据更新到自己的 L1 Cache.

总线嗅探方法很简单, CPU 需要每时每刻监听总线上的一切活动,但是不管别的核心的 Cache 是否缓存相同的数据,都需要发出一个广播事件,这无疑会**加重总线的负载**.

另外,总线嗅探只是保证了某个 CPU 核心的 Cache 更新数据这个事件能被其他 CPU 核心知道,但是**并不能保证事务串形化**.

于是,有一个协议基于总线嗅探机制实现了事务串形化,也用状态机机制降低了总线带宽压力,这个协议就是 MESI 协议

## MESI协议

MESI首先规定了缓存行(cache line)的四种状态, 可以用 2 bits 来区分:

- `Modified`: 已修改, 指数据被修改后保存在 cache line 中, 和内存中的数据不一致, 数据只存在于本 cache line 中
- `Exclusive`: 独占, 指数据和内存中的数据一致, 数据只存在于本 cache line 中
- `Shared`: 共享, 指数据和内存中的数据一致, 数据存在于很多 cache 中
- `Invalidated`: 已失效, 指该 cache line 无效

## 参考

- [一小时,完全搞懂 cpu 缓存一致性](https://zhuanlan.zhihu.com/p/651732241)
- [无锁编程_从CPU缓存一致性讲到内存模型](https://zhuanlan.zhihu.com/p/642416997)
- [在线体验 MESI 协议状态转换](https://www.scss.tcd.ie/Jeremy.Jones/VivioJS/caches/MESIHelp.htm)
- [MESI保证了缓存一致性,那么为什么多线程 i++还会有问题?的回答](https://www.zhihu.com/question/619301632/answer/3184265150)
- [Course 4 Multicore Architectures](https://www.youtube.com/playlist?list=PLeWkeA7esB-OgNoVkE2lW2cVBxpDbu92h)
  - [MESI and MOESI Protocols](https://www.youtube.com/watch?v=nrzT044qNIc)